# S3 - Simple Storage Solution

## Overview

- main building blocks of AWS
- advertised as "infinitely scaling" storage
- many websites use S3 as a backbone
- many AWS services use S3 as an integration as well

## Use Cases

- backup and storage
- distaster recovery
- archive
- hybrid cloud storage
- media hosting
- data lakes and big data analytics
- software delivery
- static website

### Buckets

- store objects (files) in buckets (directories)
- must have globally unique names (across all regions and all accounts)
- defined at the region level
- S3 looks like a global service but buckets are created in a region
- naming convention
  - no uppercase
  - no underscore
  - 3-63 characters
  - not an IP
  - start with lowercase or number
  - must NOT start with the prefix "xn--"
  - must NOT end iwth suffic "-s3alias"

### Objects

- have a key
- key is the FULL path
  - s3://mybucket/my_file.txt
  - s3://mybucket/my_folder1/another_folder/my_file.txt
- key is composed of prefix + object name
  - s3:// + [bucket name]/ + [file path]/ + [file name]
- there is no concept of directories within buckets (UI displays that way)
  - just keys with long names that contain slashes
- object values are the content of the body
  - max size is 5TB
  - if uploading more than 5GB, must use "multi-part upload"
- metadata: list of text key/value pairs - system or user metadata
- tags: unicode key/value pair up to 10 characters - useful for security / lifecycle
- Version ID: id of file if versioning is enabled

## Bucket Policy

- User-Based
  - IAM Policies - which API calls should be allowed for a specific use from IAM
- Resource-Based
  - Bucket Policies - bucekt-wide rules from the S3 console - allows cross-account access
  - Object Access Control List (ACL) - finer grain (can be disabled)
  - Bucket Access Control List (ACL) - less common (can be disabled)
- NOTE: AN IAM principal can access an S3 object if
  - the user IAM permissions *ALLOW* it **OR** the resource policy *ALLOWS* is
  - **AND** there is no explicit *DENY*
- Encryption: encruyp objects in S3 using encryption keys
- Bucket Policies
  - JSON based policy
  - Resources: bucket and objects
  - Effect: Allow/Deny
  - Actions: Set of API to Allow/Deny
  - Principal: The account or user to apply the policy to
- uses cases
  - grant public access to the bucket
  - force objects to be encrypted at upload
  - grant access to another account (Cross Account)
- Block public access settings
  - created to prevent company data leaks
  - leave these on if the bucket should never be public (overrides policy)
  - can be set at account level

## Static Website Hosting

- can host static websites and have them accessible on the internet
- URL depends on region
  - <http://bucket-name.s3-website-aws-region.amazonaws.com>
  - <http://bucket-name.s3-website.aws-region.amazonaws.com>
- if you get a 503 Forbidden error, make sure the bucket policy allows public reads.

## Versioning

- version files in S3
- enabled at the **bucekt** level
- same key overwrite will change the version
- best practice to version buckets
  - protect against unintended deletes (ability to restore a version)
  - easy to rollback to a previous version
- any file not versioned prior to enabled versioning will have version "null"
- suspending versioning does not delete previous versions

## Replication

- must enable versioning in source and destination buckets
- Cross-Region Replication (CRR)
- Same-Region Replication (SRR)
- buckets can be in different AWS accounts
- copying is asynchronous
- must give proper IAM permissions to S3
- use case
  - CRR: compliance, lower latency access, replication across accounts
  - SRR: log aggregation, live replication between production and test accounts
- after you enable replication, only new objects are replicated
- optionally tou can replicate existing objects using S3 Batch Replication
  - replicates existing objects and objects that failed replication
- for **DELETE** operations
  - can replicate delete markers from source to target (optional)
  - deletions with a version ID are **not replicated** (to avoid malicious deletes)
- there is no "chaining" of replication
  - if Bucket 1 has replication into Bucket 2, which has replication to Bucket 3, then objects created in Bucket 1 are **not replicated** into Bucket 3

## Storage Classes

- can move between storage classes manually or thru lifecycle configurations
- Durability & Availability
  - Durability
    - high durability (99.999999999%) of objects across multiple AZ
    - if you store 10,000,000 with S3 you can expect to incur a lost of a single object once every 10,000 years
    - same across all classes
  - Availablity
    - measures how readily available a service is
    - varies depending on storage class
    - Standard 99.99% = not available 53 min a year

- Standard (General Purpose)
  - 99.99% Availability
  - use for frequently accessed data
  - low latency and high throughput
  - sustain 2 concurrent facility failures
  - uses: big data analytics, mobile & gaming applications, content distrobution
- Infrequent Access
  - for data that is less frequently accessed, but requires rapid access when needed
  - lower cost than standard S3
  - Standard-Infrequent Access (S3 Standard-IA)
    - 99.9 Availability
    - uses: disaster recover, backups
  - One Zone-Infrequent Access (S3 One Zone-IA)
    - high durability in a single AZ, data lost when AZ is destroyed
    - 99.5 Availability
    - uses: secondary backup copies of on-premise data or data you can recreate
- Glacier
  - low-cost object storgae for archiving/backup
  - price for storage & object retrieval cost
  - Glacier Instant Retrieval
    - millisecond retrieval, great for data accessed once a quarter
    - minimum storage duration of 90 days
  - Glacier Flexible Retrieval
    - Expedited (1-5 min), Standard (3-5 hours), Bulk (5-12 hours) - free
    - minimum storage duration of 90 days
  - Glacier Deep Archive
    - Standard (12 hours), Bulk (48 hours)
    - minimum storage duration of 180 days
- Intelligent Tiering
  - small monthly monitoring and auto-tiering fee
  - moves objects automatically between Access Tiers based on usage
  - there are no retrieval charges
  - Tiers
    - Fequent (automatic): default tier
    - Infrequent (automatic): objects not accessed for 30 days
    - Archive Instance (automatic): objects not accessed for 90 days
    - Archive (optional): configurable from 90 to 700+ days
    - Deep Archive (optional): configurable from 180 to 700+ days

|                              |   Standard    | Intelligent Tiering |   Standard-IA    |   One Zone-IA    | Glacier Instant Retrieval | Glacier Flexible Retrieval | Glacier Deep Archive |
| :--------------------------- | :-----------: | :-----------------: | :--------------: | :--------------: | :-----------------------: | :------------------------: | :------------------: |
| Durability                   | 99.999999999% |    99.999999999%    |  99.999999999%   |  99.999999999%   |       99.999999999%       |       99.999999999%        |    99.999999999%     |
| Availability                 |    99.99%     |        99.9%        |      99.9%       |      99.5%       |           99.9%           |           99.99%           |        99.99%        |
| Availability SLA             |     99.9%     |         99%         |       99%        |       99%        |            99%            |           99.9%            |        99.9%         |
| Availability Zones           |      >=3      |         >=3         |       >=3        |        1         |            >=3            |            >=3             |         >=3          |
| Min. Storage Duration Charge |     None      |        None         |     30 Days      |     30 Days      |          90 Days          |          90 Days           |       180 Days       |
| Min. Billable Object Size    |     None      |        None         |      128KB       |      128KB       |           128KB           |            40KB            |         40KB         |
| Retrieval Fee                |     None      |        None         | Per GB Retrieved | Per GB Retrieved |     Per GB Retrieved      |      Per GB Retrieved      |   Per GB Retrieved   |

## Lifecycle Rules

- transition objects between storage classes
- **Transition Action** - configure objects to transition to anther storage class
  - move objects to Standard IA class 60 days after creation
  - move to Glacier for archiving after 6 months
- **Expiration Actions** - configure objects to expire (delete) after some time
  - access log files can be set to delete after 365 days
  - can be used to delete old versions of files (if versioning is enabled)
  - can be used to delete incomplete Multi-Part Uploads
- rules can also be created for a certain prefix
- rules can also be created for a certain object tags
- S3 Analytics
  - .csv report
  - help decide when to transition objects to the right storage class
  - recommendations for Standard and Standard IA
    - does not work for One-Zone IA or Glacier
  - report is updated daily
  - 24-48 hours to see data analysis
  - good first step to create Lifecycle Rules (or update them)

## Requester Pays

- Requestor pays for networking costs
  - must be authenticated in AWS (cannot be anonymous)
  - request cost and upload/download cost
- Owner still pays for storage costs
- helpful for when you want to share large datasets with other accounts

## Event Notifications

- Created, Removed, Restored, Replicated
- object name filtering
- use case: generate thumbnails of images uploaded to S3
- create as many events as desired
- typically happen in seconds but sometimes can take a minute or longer
- All events => EventBridge => over 18 services
  - advanced filtering options with JSON rules (metadata, object size, name)
  - multiple destinations - ex step functions, kinesis streas/firehose
  - EventBridge Capabilites - Archive, Replay Events, Reliable Delivery

## Perfomance

- automatically scales to high request rate, 100-200ms latency
- aplication can at achieve at least 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket
- no limit to the number of prefixes
  - example (object path => prefix)
    - bucket/folder1/sub1/file => /folder1/sub1/
    - bucket/folder1/sub2/file => /folder1/sub2/
    - bucket/1/file => /1/
    - bucket/2/file => /2/
  - if you spread reads across all four prefixes evenly you can achieve 22,000 GET/HEAD requests per second
- **Multi-Part Upload**
  - recommended for files > 100MB
  - required for files > 5GB
  - can help parallelize uploads (speed up transfers)
- **S3 Transfer Acceleration**
  - increase transfer speed by transferring file to and AWS edge location which will forward the data to the S3 bucket in the target region
  - ~200 edge locations
  - usa --public www network--> usa edge --private aws network--> australia
- **S3 Byte-Range Fetches**
  - Parallelize GETS by requesting specific byte ranges
  - better resilience in case of failures
  - can be used to speed up downloads
  - can be used to retrieve only partial data (e.g., head of the file)

## Select & Glacier Select

- retrieve less data using SQL by performing server-side filtering
- can filter by rows & columns (simple SQL statements)
- less network transfer, less CPU cost client-side

## Batch Operations

- perfom bulk operations on exisiting objects with a single request
  - modify object metadata & properties
  - copy objects between buckets
  - **Encrypt un-encrypted objects**
  - modify ACLs, tags
  - restore objects from S3 Glacier
  - invoke Lambda function to perfom custom action on each object
- a job consists of a list of objects, the action to perfom, and optional parameters
- manages retries, tracks progress, sends completion notifications, generates reports
- **You can use S3 Inventory to get object list and use S3 Select to filter your objects**
