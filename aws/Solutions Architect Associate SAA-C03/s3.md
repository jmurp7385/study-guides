# S3 - Simple Storage Solution

## Overview

- main building blocks of AWS
- advertised as "infinitely scaling" storage
- many websites use S3 as a backbone
- many AWS services use S3 as an integration as well

## Use Cases

- backup and storage
- distaster recovery
- archive
- hybrid cloud storage
- media hosting
- data lakes and big data analytics
- software delivery
- static website

### Buckets

- store objects (files) in buckets (directories)
- must have globally unique names (across all regions and all accounts)
- defined at the region level
- S3 looks like a global service but buckets are created in a region
- naming convention
  - no uppercase
  - no underscore
  - 3-63 characters
  - not an IP
  - start with lowercase or number
  - must NOT start with the prefix "xn--"
  - must NOT end iwth suffic "-s3alias"

### Objects

- have a key
- key is the FULL path
  - s3://mybucket/my_file.txt
  - s3://mybucket/my_folder1/another_folder/my_file.txt
- key is composed of prefix + object name
  - s3:// + [bucket name]/ + [file path]/ + [file name]
- there is no concept of directories within buckets (UI displays that way)
  - just keys with long names that contain slashes
- object values are the content of the body
  - max size is 5TB
  - if uploading more than 5GB, must use "multi-part upload"
- metadata: list of text key/value pairs - system or user metadata
- tags: unicode key/value pair up to 10 characters - useful for security / lifecycle
- Version ID: id of file if versioning is enabled

## Bucket Policy

- User-Based
  - IAM Policies - which API calls should be allowed for a specific use from IAM
- Resource-Based
  - Bucket Policies - bucekt-wide rules from the S3 console - allows cross-account access
  - Object Access Control List (ACL) - finer grain (can be disabled)
  - Bucket Access Control List (ACL) - less common (can be disabled)
- NOTE: AN IAM principal can access an S3 object if
  - the user IAM permissions *ALLOW* it **OR** the resource policy *ALLOWS* is
  - **AND** there is no explicit *DENY*
- Encryption: encruyp objects in S3 using encryption keys
- Bucket Policies
  - JSON based policy
  - Resources: bucket and objects
  - Effect: Allow/Deny
  - Actions: Set of API to Allow/Deny
  - Principal: The account or user to apply the policy to
- uses cases
  - grant public access to the bucket
  - force objects to be encrypted at upload
  - grant access to another account (Cross Account)
- Block public access settings
  - created to prevent company data leaks
  - leave these on if the bucket should never be public (overrides policy)
  - can be set at account level

## Static Website Hosting

- can host static websites and have them accessible on the internet
- URL depends on region
  - <http://bucket-name.s3-website-aws-region.amazonaws.com>
  - <http://bucket-name.s3-website.aws-region.amazonaws.com>
- if you get a 503 Forbidden error, make sure the bucket policy allows public reads.

## Versioning

- version files in S3
- enabled at the **bucekt** level
- same key overwrite will change the version
- best practice to version buckets
  - protect against unintended deletes (ability to restore a version)
  - easy to rollback to a previous version
- any file not versioned prior to enabled versioning will have version "null"
- suspending versioning does not delete previous versions

## Replication

- must enable versioning in source and destination buckets
- Cross-Region Replication (CRR)
- Same-Region Replication (SRR)
- buckets can be in different AWS accounts
- copying is asynchronous
- must give proper IAM permissions to S3
- use case
  - CRR: compliance, lower latency access, replication across accounts
  - SRR: log aggregation, live replication between production and test accounts
- after you enable replication, only new objects are replicated
- optionally tou can replicate existing objects using S3 Batch Replication
  - replicates existing objects and objects that failed replication
- for **DELETE** operations
  - can replicate delete markers from source to target (optional)
  - deletions with a version ID are **not replicated** (to avoid malicious deletes)
- there is no "chaining" of replication
  - if Bucket 1 has replication into Bucket 2, which has replication to Bucket 3, then objects created in Bucket 1 are **not replicated** into Bucket 3

## Storage Classes

- can move between storage classes manually or thru lifecycle configurations
- Durability & Availability
  - Durability
    - high durability (99.999999999%) of objects across multiple AZ
    - if you store 10,000,000 with S3 you can expect to incur a lost of a single object once every 10,000 years
    - same across all classes
  - Availablity
    - measures how readily available a service is
    - varies depending on storage class
    - Standard 99.99% = not available 53 min a year

- Standard (General Purpose)
  - 99.99% Availability
  - use for frequently accessed data
  - low latency and high throughput
  - sustain 2 concurrent facility failures
  - uses: big data analytics, mobile & gaming applications, content distrobution
- Infrequent Access
  - for data that is less frequently accessed, but requires rapid access when needed
  - lower cost than standard S3
  - Standard-Infrequent Access (S3 Standard-IA)
    - 99.9 Availability
    - uses: disaster recover, backups
  - One Zone-Infrequent Access (S3 One Zone-IA)
    - high durability in a single AZ, data lost when AZ is destroyed
    - 99.5 Availability
    - uses: secondary backup copies of on-premise data or data you can recreate
- Glacier
  - low-cost object storgae for archiving/backup
  - price for storage & object retrieval cost
  - Glacier Instant Retrieval
    - millisecond retrieval, great for data accessed once a quarter
    - minimum storage duration of 90 days
  - Glacier Flexible Retrieval
    - Expedited (1-5 min), Standard (3-5 hours), Bulk (5-12 hours) - free
    - minimum storage duration of 90 days
  - Glacier Deep Archive
    - Standard (12 hours), Bulk (48 hours)
    - minimum storage duration of 180 days
- Intelligent Tiering
  - small monthly monitoring and auto-tiering fee
  - moves objects automatically between Access Tiers based on usage
  - there are no retrieval charges
  - Tiers
    - Fequent (automatic): default tier
    - Infrequent (automatic): objects not accessed for 30 days
    - Archive Instance (automatic): objects not accessed for 90 days
    - Archive (optional): configurable from 90 to 700+ days
    - Deep Archive (optional): configurable from 180 to 700+ days

|                              |   Standard    | Intelligent Tiering |   Standard-IA    |   One Zone-IA    | Glacier Instant Retrieval | Glacier Flexible Retrieval | Glacier Deep Archive |
| :--------------------------- | :-----------: | :-----------------: | :--------------: | :--------------: | :-----------------------: | :------------------------: | :------------------: |
| Durability                   | 99.999999999% |    99.999999999%    |  99.999999999%   |  99.999999999%   |       99.999999999%       |       99.999999999%        |    99.999999999%     |
| Availability                 |    99.99%     |        99.9%        |      99.9%       |      99.5%       |           99.9%           |           99.99%           |        99.99%        |
| Availability SLA             |     99.9%     |         99%         |       99%        |       99%        |            99%            |           99.9%            |        99.9%         |
| Availability Zones           |      >=3      |         >=3         |       >=3        |        1         |            >=3            |            >=3             |         >=3          |
| Min. Storage Duration Charge |     None      |        None         |     30 Days      |     30 Days      |          90 Days          |          90 Days           |       180 Days       |
| Min. Billable Object Size    |     None      |        None         |      128KB       |      128KB       |           128KB           |            40KB            |         40KB         |
| Retrieval Fee                |     None      |        None         | Per GB Retrieved | Per GB Retrieved |     Per GB Retrieved      |      Per GB Retrieved      |   Per GB Retrieved   |

## Lifecycle Rules

- transition objects between storage classes
- **Transition Action** - configure objects to transition to anther storage class
  - move objects to Standard IA class 60 days after creation
  - move to Glacier for archiving after 6 months
- **Expiration Actions** - configure objects to expire (delete) after some time
  - access log files can be set to delete after 365 days
  - can be used to delete old versions of files (if versioning is enabled)
  - can be used to delete incomplete Multi-Part Uploads
- rules can also be created for a certain prefix
- rules can also be created for a certain object tags
- S3 Analytics
  - .csv report
  - help decide when to transition objects to the right storage class
  - recommendations for Standard and Standard IA
    - does not work for One-Zone IA or Glacier
  - report is updated daily
  - 24-48 hours to see data analysis
  - good first step to create Lifecycle Rules (or update them)

## Requester Pays

- Requestor pays for networking costs
  - must be authenticated in AWS (cannot be anonymous)
  - request cost and upload/download cost
- Owner still pays for storage costs
- helpful for when you want to share large datasets with other accounts

## Event Notifications

- Created, Removed, Restored, Replicated
- object name filtering
- use case: generate thumbnails of images uploaded to S3
- create as many events as desired
- typically happen in seconds but sometimes can take a minute or longer
- All events => EventBridge => over 18 services
  - advanced filtering options with JSON rules (metadata, object size, name)
  - multiple destinations - ex step functions, kinesis streas/firehose
  - EventBridge Capabilites - Archive, Replay Events, Reliable Delivery

## Perfomance

- automatically scales to high request rate, 100-200ms latency
- aplication can at achieve at least 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket
- no limit to the number of prefixes
  - example (object path => prefix)
    - bucket/folder1/sub1/file => /folder1/sub1/
    - bucket/folder1/sub2/file => /folder1/sub2/
    - bucket/1/file => /1/
    - bucket/2/file => /2/
  - if you spread reads across all four prefixes evenly you can achieve 22,000 GET/HEAD requests per second
- **Multi-Part Upload**
  - recommended for files > 100MB
  - required for files > 5GB
  - can help parallelize uploads (speed up transfers)
- **S3 Transfer Acceleration**
  - increase transfer speed by transferring file to and AWS edge location which will forward the data to the S3 bucket in the target region
  - ~200 edge locations
  - usa --public www network--> usa edge --private aws network--> australia
- **S3 Byte-Range Fetches**
  - Parallelize GETS by requesting specific byte ranges
  - better resilience in case of failures
  - can be used to speed up downloads
  - can be used to retrieve only partial data (e.g., head of the file)

## Select & Glacier Select

- retrieve less data using SQL by performing server-side filtering
- can filter by rows & columns (simple SQL statements)
- less network transfer, less CPU cost client-side

## Batch Operations

- perfom bulk operations on exisiting objects with a single request
  - modify object metadata & properties
  - copy objects between buckets
  - **Encrypt un-encrypted objects**
  - modify ACLs, tags
  - restore objects from S3 Glacier
  - invoke Lambda function to perfom custom action on each object
- a job consists of a list of objects, the action to perfom, and optional parameters
- manages retries, tracks progress, sends completion notifications, generates reports
- **You can use S3 Inventory to get object list and use S3 Select to filter your objects**

## Security

### Encryption

- 4 methods of object encryption
- important to know which ones are for which situation for the exam
- Server-Side Encryption (SSE)
  - Server-Side Encryption with Amazonm S3-Managed Keys (SSE-S3)
    - Encrypts S3 objects using keys handled, managed, and owned by AWS
    - object is server-side
    - encryption type is AES-256
    - must set headser "x-amz-server-side-encryption":"AES256"
  - Server-Side Encryption with KMS Keys stored in AWS KMS (SSE-KMS)
    - Leverages AWS Key Management Service (AWS KMS) to manage encryption keys
    - user control + audit key usage in Cloud Trial
    - must set headser "x-amz-server-side-encryption":"aws:kms"
    - Limitations
      - may be imapacted by KMS limits
      - when you upload, it calls the GenerateDataKey KMS API
      - when you download, it calls the Decrypt KMS API
      - Counts towards KMS quota per second
        - 5500, 10000, 30000 req/s based on region
        - can request a quota increase using the Sevice Quotas Console
        - might have a throttling use-case on exam
  - Server-Side Encryption with Customer-Provided Keys (SSE-C)
    - when you want to manage your own encryption keys
    - does not store keys provided
    - HTTPS must be used
    - Encryption key must be provided in HTTP headers for every HTTP request
- Client-Side Encyption
  - use client libraries such as Amazon S3 Client-Side Encryption Library
  - clients must encrypt data themselves before sending to S3
  - clients must decrupt data themselves when retrieving from S3
  - customer fully manages the keys and encryption cycle
- Encryption in transit (SSL/TLS)
  - S3 exposes 2 endpoints
    - HTTP - non-encrypted
    - HTTPS - encryption in flight
  - HTTPS is recommended
  - HTTPS is mandatory for SSE-C
  - most clients would use the HTTPS endpoint by default
- Default Encryption vs Bucket Policies
  - one way to "force encyption" is to use a bucket policy and refuse any API call to PUT an S3 object without enncryption headers
  - another way is to uyse the "default encryption" option in S3
  - Bucket Policies are evaluated before "default encryption"

### CORS

- cross-origin resource sharing
- popular exam question
- Origin = scheme (protocol) + host (domain) + port
  - <https://www.example.com>
    - scheme = HTTPS
    - domain = www.example.com
    - port = implied 443 for HTTPS, 80 for HTTP
- same origin: <http://example.com/app1> and <http://example.com/app2>
- different origins: <http://example.com> and <http://other.example.com>
- The request won't be fufilled unless the other origin alows for the requests, using CORS Headers (ex: **Access-Control-Allow-Origin**)
- if a client makes a cross-origin request on our S3 bucekt, we need to enable to the correct CORS Headers
- can allow a specific origin or "*" for all origins

### MFA Delete

- MFA (Multi-Factor Authentication) - force users to generate a code on a device (usually a mobile phone or hardware) before doing important operations on S3
- MFA will be required to
  - permanently delete and object version
  - suspend versioning on a bucket
- MFA **won't** be required to
  - enable versioning
  - list deleted versions
- To use MFA Delete, Versioning **must be enabled** on the bucket
- **only the bucket owner (root account) can enable/disable MFA Delete**

### Access Logs

- for audit purposes, you may want to log all access to S3 buckets
- any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket
  - can be analyzed using data analysis tools
    - Amazon Athena
  - target logging bucket must be in the same AWS Region
- **Do not set your logging bucket to be the monitoring bucket**
  - it will create a logging loop and **your bucket will grow exponentially**

### Pre-signed URLs

- generate pre-signed URLs using the S3 Console, CLI, or SDK
- URL Expiration
  - S3 console - 1 min up to 720 min (12 hours)
  - AWS CLI - configure expiration with --expiration parameter in seconds
    - default 3600 seconds, max. 604800 seconds ~168 hours
- users given a pre-signed URL inherit the permissions of the user that generated the URL for GET/PUT
- examples
  - Allow only logged-in users to download a premium video from your S3 bucket
  - Allow an ever changing list of users to download files by generating URLs dynamically
  - Allow a user **temporarily** to upload a file to a precise location in your S3 bucket

### Glacier Vault Lock & S3 Object Lock

- Glacier Vault Lock
  - adopt a WORM (WRite Once Read Many) model
  - create a vault lock policy
  - lock the policy for future edits (can no longer be changed or deleted)
  - helpful for compliance and data retention
- S3 Object Lock
  - adopt a WORM (WRite Once Read Many) model
  - block an object version deletion for a specified amount of time
  - Rention mode - compliance
    - object versions can't be overwritted or deleted by any user, including the root user
    - object retention modes can't be changed, and retention periods can't be shortened
  - Retention mode - governance
    - most users can't overwrite or delete an object version or alter its lock settings
    - some users have special permissions to change the retention or delete the object
  - Retention Period - protect the object for a fixed period of time, it can be extended
  - Legal Hold
    - protect and object indefinitely, independant from retention period
    - can be freely placed and removed using the s3:PutObjectLegalHold IAM permission

### S3 Access Points & Object Lambda

- Each Access Point gets its own DNS and policy to limit who can access it
  - a specific IAM user/group
  - one policy per Access Point => **Easier to manage complex bucket policies**
- S3 Object Lambda
  - use AWS Lambda Functions to change the object before it is retrieved by the caller application
  - only one S3 bucket is needed, on top of which we create **S3 Access Point** and **S3 Object Lambda Access Points**
  - use cases
    - redacting personally identifiable information for analytics or non-production environments
    - converting across data formats, XML -> JSON
    - resizing and watermarking images on the fly using caller-specific details, such as the user who requested the object
